---
layout: post
title: 全连接神经网络
draft: false
description: '全连接神经网络'
tags:
  - CV
  - AI
categories:
  - CV
mathjax: true
abbrlink: fd2893d1
date: 2020-09-13 08:38:18
---

## 图像表示

直接利用原始像素作为特征，展开为列向量。CIFAR10中为3072维。

## 分类模型

### 多层感知器

全连接神经网络级联多个变换来实现输入到输出的映射。

两层全连接网络：$f = W_2 max(0, W_1x + b_1) + b_2$

同样三层全连接网络：$f = W_3 max(0,W_2 max(0, W_1x + b_1) + b_2) + b_3$

非线性操作$max$不可以被去掉，去掉则变成了线性的。

线性分类器中的$W$可看作模板，模板的个数由类别个数决定；  
全连接神经网络中，$W_1$也可以看作模板，模板个数人为指定，可以用于拆分或组合模板类。但最外的$W$的个数由类别决定，用于融合多个模板的匹配结果来实现最终类别打分。

线性分类器能解决线性可分的问题，但实际问题通常不是线性可分的。全连接神经网络分界面是非线性的。

### 激活函数

为什么需要非线性操作？  
如果网络中缺少激活函数，全连接神经网络将变成一个线性分类器。

常用激活函数：

![激活函数](/images/CV/NeuralNetwork_1.png)

## 损失函数 

### SOFTMAX损失与交叉熵

SOFTMAX？

![SOFTMAX](/images/CV/NeuralNetwork_2.png)

SOFTMAX操作之后的输出层是一个概率分布。

交叉熵损失？

分类器预测分布$q(x)$，真实分布$p(x)$【只有一个值是1，其他类别为0，此类分布称为one-hot分布】。

交叉熵则用来度量预测分布与真实分布之间的差别，用于衡量预测的准确率。

熵： $H(p) = - \sum_x p(x) log p(x)$

交叉熵： $H(p, q) = - \sum_x p(x) log q(x)$

相对熵： $KL(p || q) = - \sum_x p(x) log \frac{q(x)}{p(x)}$

相对熵(`relatice entropy`)也叫KL散度(`KL divergence`)；用来度量两个分布之间的不相似性。相对熵不能称为距离，不具备对称性。

$$
\begin{align}
H(p, q) &= - \sum_x p(x) log q(x) \\\\\\
&= -\sum_x p(x) log p(x) - \sum_x p(x) log \frac{q(x)}{p(x)} \\\\\\
&= H(p) + KL(p || q)
\end{align}
$$

由于真实分布是`one-hot`形式，所以对应的交叉熵损失直接为$-log(q_j)$，其中$j$为真实类别。

### 对比多类支持向量机损失

相同分数下两种分类器的损失有什么区别？

损失变化不大却出现精度变化很大情况，在于细微差别得分下【最大得分维度改变】交叉熵损失变化不大。

对于多类支持向量机损失，只要求最大的比其他大超过1则能保证损失为0，但对于交叉熵损失不仅需要最大，还要求大很多才能使损失低。

## 优化算法

### 计算图与反向传播

什么是计算图？

计算图是一种有向图，它用来表达数入、输出以及中间变量之间的计算关系，图中的每个节点对应着一中数学运算。

![计算图的前反向计算](/images/CV/NeuralNetwork_3.png)

前向计算得出的是函数值，反向计算得出的是梯度值。

计算图总结：

任意复杂的函数，都可以用计算图的形式表示；  
再整个计算图中，每个门单元都会得到一些输入，然后进行下面两个计算：a)这个门的输出值，b)其输出值关于输入值的局部梯度；  
利用链式法则，门单元应该将回传的梯度诚意它对其的输入的局部梯度，从而得到整个网络的输出对该门单元的每个输入值的梯度。

### 再谈激活函数

Sigmoid激活函数：$\sigma(x) = \frac{1}{1+e^{-x}}$

![Sigmoid函数](/images/CV/NeuralNetwork_4.png)

当输入值大于10或小于-10时，局部梯度都是0；非常不利于网络的梯度流传递。即出现**梯度消失**现象。

梯度消失时神经网络训练中非常致命的一个问题，其本质时由于链式法则的乘法特性导致梯度为0，无法进行优化。

与之对应的是**梯度爆炸**，也是由于链式法则的乘法特性导致的。

梯度爆炸：断崖处梯度(梯度值很大)诚意学习率后会是一个非常大的值，从而“飞”除了合理区域，最终导致算法不收敛；  
解决方案：把沿梯度方向前进的步长限制再某个值内就可以避免“飞”出了，这个方法也成为**梯度裁剪**。

ReLU激活函数：$f(x) = max {0, x}$

![ReLu函数](/images/CV/NeuralNetwork_5.png)

当输入大于0时，局部梯度永远不会为0，比较有利于梯度流的传递。

Leakly ReLU激活函数：$f(x) = max {0.1x, x}$

![Leakly ReLu函数](/images/CV/NeuralNetwork_6.png)

基本没有“死区”，也就是梯度永远不会为0，之所以说“基本”，是因为函数在0处没有导数。

### 动量法与自适应梯度

#### 梯度下降算法存在的问题

损失函数特性：一个方向上变化迅速而在另一个方向上变化缓慢。  
优化目标：从起点处走到底端。  
梯度下降算法存在的问题：山壁间震荡，往谷底方向的行进较慢。仅增大步长并不能加快算法收敛速度。

#### 动量法

目标：改进梯度下降算法存在的问题，即减少震荡，加速通往谷底。  
改进思想：利用累加历史梯度信息更新梯度。

![动量法](/images/CV/NeuralNetwork_7.png)

累加过程中震荡方向相互抵消，平坦方向得到加强。

![动量法的效果](/images/CV/NeuralNetwork_8.png)

损失函数通常具有不太好的局部最小值或鞍点，梯度下降算法中局部最小处或暗点处梯度为0，算法无法通过。由于动量的存在，算法可以冲出局部最小点以及鞍点，找到最优的解。

#### 自适应梯度与RMSProp

自适应梯度法通过减小震荡方向步长，增大平坦方向步长来减少震荡，酵素通往谷底方向。

如何区分震荡方向与平坦方向？

梯度幅度的平方较大的方向是震荡方向；相反是平坦方向

AdaGrad方法是一种自适应梯度方法：

![AdaGrad方法](/images/CV/NeuralNetwork_9.png)

RMSProp方法是对AdaGrad的改进：

![RMSProp方法](/images/CV/NeuralNetwork_10.png)

$\rho$取值范围$[0,1)$，$\rho = 0$时仅考虑当前梯度的强度，建议设置$\rho = 0.999$

#### ADAM

同时使用动量和自适应梯度思想

![Adam算法](/images/CV/NeuralNetwork_11.png)

修正偏差步骤可以极大缓解算法初期的冷启动问题。

### 总结

![各算法比较](/images/CV/NeuralNetwork_12.png)

## 训练过程

### 权值初始化

全零初始化：网络中不同的神经元有相同的输出，进行同样的参数更新；因此，这些神经元学到的参数都一样，等价于一个神经元，所以需要避免全零初始化，可以采用随机初始化。

初始化让权值不相等并不能保证网络能够正常被训练，是网络各层的激活值和局部梯度的方差在传播过程中尽量保持一致，以保证网络中正向和方向数据流动。

#### Xavier初始化

一个神经元，其输入为$z_1,z_2,\cdots,z_N$，这$N$个输入时独立分布的；其权值为$w_1,w_2,\cdots,w_N$，它们也是独立同分布的，且$w$和$z$是独立的；其激活函数为$f$；其最终输出$y$的表达式：

$$
y = f(w_1 * z_1 + \cdots + w_N * z_N)
$$

目标：使网络各层的激活值和局部梯度的方差在传播过程中尽量保持一致，即寻找$w$的分布使得输出$y$与输入$z$的方差一致。

假设$f$为双曲正切函数，$w_1,\cdots,w_N$独立同分布，$z_1,\cdots,z_N$独立同分布，随机变量$w$与$z$独立，且均值都为0，则有：

$$
\begin{align}
Var(y) &= Var(\sum_{i=1}^{n} w_i z_i) = \sum_{i=1}^{n} Var(w_i z_i) \\\\\\
&= n Var(w_i) Var(z_i)
\end{align}
$$

$Var(w) = \frac{1}{N}$时，$y$的方差与$z$的方差一致。

随机初始化：权值采样自$\text{N}(0,\frac{1}{N})$的高斯分布，$N$为输入神经元个数。

好的初始化方法可以防止前向传播过程中的信息消失，也可以解决反向传递过程中的梯度消失。

激活函数选择双曲正切或者$Sigmoid$时，建议使用$Xaizer$初始化方法；  
激活函数选择$ReLU$或$Leakly ReLU$时，推荐使用$He$初始化方法。

### 批归一化

与权值初始化思路不同，批归一化直接对神经元的输出进行批归一化来保证正向和反向数据流通畅。

批归一化操作：对输出进行减增值除方差操作，可保证当前神经元的输出值的分布符合0均值1方差。

每一层的每个神经元进行批归一化，就能解决前向传递过程中信号消失问题。

![批归一化算法](/images/CV/NeuralNetwork_13.png)

前三步输出0均值1方差的正态分布为什么还要进行第四步的平移缩放？根据对分类的贡献自行决定数据分布的均值与方差。

单张样本测试时，均值和方差怎么设置？

来自于训练中，累加训练时每个批次的均值和方差，最后进行平均，用平均后的结果作为预测时的均值和方差。

### 欠拟合、过拟合与Dropout

机器学习的根本问题就是优化和泛化的问题。

优化————是指调节模型以在训练数据上得到最佳幸能；  
泛化————是指训练好的模型在前所未见的数据上的幸能好坏。

训练初期：优化和泛化事相关的；训练集上的误差越小，验证集上的误差也越小，模型的泛化能力逐渐增强。

训练后期：模型在验证集上的错误率不在降低，转而后开始升高。模型出现过拟合，开始学习仅和训练数据有关的模式。

应对过拟合：

最优方案————获取更多的训练数据；  
次优方案————调节模型允许存储的信息量或者对模型允许存储的信息加以约束，该类方法也称为正则化。

随机失活(`Dropout`)：让隐藏层的神经元以一定的概率不被激活。

实现方式：训练过程中，对某一层使用Dropout，就是随机将该层的一些输出舍弃(输入值设置为0)，这些被舍弃的神经元就好像被网络删除了一样。

随机失活比率(`Dropout ratio`)：是被设为0的特征所占的比例，通常在$[0.2, 0.5]$。

随机失活为什么能够防止过拟合？

1. 随机失活使得每次更新梯度时参与计算地网络数减少了，降低了模型容量，所以能防止过拟合。
2. 随机失活鼓励权重分散，从这个角度来看随机失活也能起到正则化的作用，进而防止过拟合。  
3. Dropout可以看作模型集成。

### 模型正则与超参数调优

超参数：

网络结构————隐藏层神经元个数、网络层数、非线性单元选择等  
优化相关————学习率、dropout比率、正则项强度等

超参数优化方法：

网络搜索法：

1. 每个超参数分别取几个值，组合这些超参数值，形成多组超参数；
2. 在验证集上评估每组超参数的模型性能；
3. 选择性能最优的模型所采用的那组值作为最终的超参数的值。

随机搜索法：

1. 参数空间内随机取点，每个点对应一组超参数；
2. 在验证集上评估每组超参数的模型性能；
3. 选择性能最优的模型所采用的那组值作为最终的超参数的值。