---
title: 非线性规划
draft: false
description: 非线性规划；
tags:
  - Convex Optimization
  - Algorithm
categories:
  - Algorithm
mathjax: true
abbrlink: 6f3e0892
date: 2020-06-16 16:04:28
---

# 非线性规划

## 定义

如果目标函数或约束条件中包含非线性函数，就称这种规划问题为非线性规划(`Nonlinear Programming`)问题。

一般来说，解非线性规划要比解线性规划问题困难的多。而且不像线性规划中有单纯形法这一通用方法，非线性规划目前还无通用解法，各个方法都有特定的使用条件。

其一般形式为：

$$
\begin{array}{ll}
\text{min  } & f(x)  \\\\\\
\text{s.t.}  & h_j(x) \leq 0, \text{  } j = 1,\cdots,q \\\\\\
 & g_i(x) = 0, \text{  } i = 1,\cdots,p 
\end{array} \tag {NP}
$$

其中$x=[x_1 \cdots x_n]^T$称为模型的决策变量，$f$称为目标函数，$g_i(i=1,\cdots,p)$和$h_j(j=1,\cdots,q)$称为约束函数。另外目标函数或者约束函数之间至少有一个是非线性函数。

## 与线性规划的区别

如果线性规划的最优解存在，其最优解智能在其可行域的边界上达到；而非线性规划的最优解(如果最优解存在)则可能在其可行域的任意一点达到。

## 求解非线性规划的基本迭代格式

记非线性规划问题(NP)的可行域为$K$。

若$x^{\*} \in K$，并且$f(x^{\*}) \leq f(x), \forall x \in K$，则称$x^{\*}$是(NP)的整体最优解，$f(x^{\*})$是(NP)的整体最优值。

如果有$f(x^{\*}) \lt f(x), \forall x \in K, x\not ={x^{\*}}$，则称$x^{\*}$是(NP)的严格整体最优解，$f(x^{\*})$是(NP)的严格整体最优值。

若$x^{\*} \in K$，并且存在$x^{\*}$的邻域$N_{\delta}(x^{\*})$，使$f(x^{\*}) \leq f(x), \forall x \in N_{\delta}(x^{\*})\cap K$，则称$x^{\*}$是(NP)的局部最优解，$f(x^{\*})$是(NP)的局部最优值。

如果有$f(x^{\*}) \lt f(x), \forall x \in N_{\delta}(x^{\*})\cap K$，则称$x^{\*}$是(NP)的严格局部最优解，$f(x^{\*})$是(NP)的严格局部最优值。

由于线性规划的目标函数为线性函数，可行域为凸集，因而求出的最优解就是整个可行域上的全局最优解。而在非线性规划中有时求出的某个解虽是一部分可行域上的极值点，但却并不一定是整个可行域上的全局最优解。

对于非线性规划模型，可以采用迭代方法求它的最优解。

迭代方法的基本思想是：

从一个选定的初始点$x^0 \in R^n$出发，按照某一特定的迭代规则产生一个点列${x^k}$，使得当{x^k}是有穷点列时，其最后一个点是(NP)的最优解；当${x^k}$是无穷点列时，它有极限点，并且其极限点是(NP)的最优解。

设$x^k \in R^n$是某迭代方法的第$k$轮迭代点，$x^{k+1} \in R^n$是第$k+1$轮迭代点，则有基本迭代格式：

$$
x^{k+1} = x^{k} + t_k p^k \tag{1}
$$

这里$t_k$是一个系数，$p^k \in R^n, ||p^k|| = 1$，并且$p^k$的方向是从点$x^k$向着点$x^{k+1}$的方向。

通常，我们把基本迭代格式中的$p^k$称为第$k$轮搜索方向，$t_k$为沿$p^k$方向的步长，使用迭代方法求解(NP)的关键在于，如何构造每一轮的搜索方向和确定适当的步长。

设$\hat x \in R^n, p \not ={0}$，若存在$\delta \gt 0$，使$f(\hat x + tp) \lt f(\hat x), \forall t \in (0,\delta)$，称向量$p$是$f$，在点$\hat x$处的**下降方向**。

设$\hat x \in R^n, p \not ={0}$，若存在$t \gt 0$，使$\hat x + tp \in K$，称向量p是点$\hat x$处关于$K$的**可行方向**。

一个向量$p$，若及时函数$f$在点$\hat x$处的下降方向，又是该点关于区域$K$的可行方向，称之为函数$f$在点$\hat x$处关于$K$的可行下降方向，即基本迭代式中的$p^k$。

则有基于基本迭代式求解(NP)的一般步骤：

0. 选取初始点$x^0$，令$k:=0$;
1. 构造搜索方向：依照一定规划，构造$f$在点$x^k$处关于$K$的可行下降方向作为搜索方向$p^k$；
2. 寻求搜索步长：以$x^k$为起点沿搜索方向$p^k$寻求适当的步长$t_k$，使目标函数值有某种意义的下降；
3. 求出下一个迭代点：$x^{k+1} = x^{k} + t_k p^k$，若$x^{k+1}$已满足某种终止条件，停止迭代；
4. 以$x^{k+1}$代替$x^k$，回到第1步。

# 无约束问题

## 一维搜索法

一维搜索法即沿某一已知方向求目标函数的极小点。一维搜索法有很多，常用的有：试探法(斐波那契法)、插值法(抛物线插值法，三次插值法)、微积分中的求根法(切线法，二分法等)。

记一维极小化问题:

$$
\underset{a \leq t \leq b}{min } f(t) \tag{2}
$$

若$f(t)$在区间$[a,b]$内是单峰函数，则基本方法是通过不断地缩短$[a,b]$的长度，来搜索得到(2)的近似最优解，基本方法如下：

```c++
while(b-a < delta){//delta是精度
    任选 t1,t2关于(a+b)/2对称;//t2 < t1
    if(f(t2) < f(t1)){
        b = t1;
    }else{
        a = t2;
    }
}
```

Fibonacci法和0.618法都是使得给定的单峰区间的长度能够尽快的缩短。

在Fibonacci法中，可以首先根据区间长度和精度计算出搜索次数：

$$
\frac{b-a}{F_n} \lt \delta
$$

其中$F_n$表示第$n$个Fibonacci数，则基本方法为：

```c++
k = 1;
t2 = a;
t1 = b;
while(k < n-1){
    if(f(t1) < f(t2)){
        a = t2;
        t2 = t1;
        t1 = a + F[n-1-k]/F[n-k] * (b - a);
    }else{
        b = t1;
        t1 = t2;
        t2 = b + F[n-1-k]/F[n-k] * (a - b);
    }
    k++;
}
```

## 二次插值法

对极小化问题(2)，当$f(t)$在$[a,b]$上连续时，可以考虑使用多项式插值来进行一维搜索。

基本思想是：在搜索区间中，不断用低次(不超过3次)多项式来近似目标函数，并逐步用插值多项式的极小点来逼近(2)的最优解。

## 无约束极值问题

无约束极值问题可表述为：

$$
\text{min } f(x), \text{ } x \in E^{(n)} \tag{3}
$$

求解该类问题的迭代法大体分为两类：一是用到函数的一阶导数或二阶导数的解析法，另一是仅用到函数值的直接法。

### 解析法

对基本迭代式(1)，根据微积分相关知识可知，点$x^k$沿其负梯度方向$p^k = - \nabla f(x^k)$方向下降最快，称之为最速下降方向。

按照基本迭代式，每一轮从点$x^k$出发沿最速下降方向作一维搜索，称之为最速下降法。

下面介绍另一种解析法——Newton 法。

考虑目标函数$f$在点$x^k$出的二次逼近式：

$$
f(x) \approx Q(x) = f(x^k) + \nabla f(x^k)^T (x - x^k) + \frac{1}{2} (x - x^k)^T \nabla ^2 f(x^k) (x - x^k)
$$

记Hesse阵：

$$
\nabla ^2 f(x^k) = 
\begin{bmatrix}
\frac{\partial ^2 f(x^k)}{\partial x_1^2} & \cdots & \frac{\partial ^2 f(x^k)}{\partial x_1 \partial x_n} \\\\\\
\vdots & \ddots & \vdots \\\\\\
\frac{\partial ^2 f(x^k)}{\partial x_n \partial x_1} & \cdots & \frac{\partial ^2 f(x^k)}{\partial x_n^2}
\end{bmatrix}
$$

假定Hesse阵正定，函数$Q$的驻点$x^{k+1}$是$Q(x)$的极小点。

为求此极小点，令$\nabla Q(x^{k+1}) = \nabla f(x^k) + \nabla ^2 f(x^k)(x^{k+1} - x^k) = 0$，解得：$x^{k+1} = x^k - [\nabla ^2 f(x^k)]^{-1} \nabla f(x^k)$。

根据基本迭代式，可知从点$x^k$出发沿搜索方向$p^k = - [\nabla ^2 f(x^k)]^{-1} \nabla f(x^k)$，并取步长$t_k = 1$即可得$Q(x)$得极小值点$x^{k+1}$。通常把方向$p^k$称为$x^k$得Newton方向。

如果目标函数是非二次函数，一般地说，用Newton 法通过有限轮迭代并不能保证可求得其最优解。

Newton 法地优点是收敛速度快；缺点是有时不好用且当维数较高时计算量非常大。

### 变尺度法

变尺度法(`Variable Metric Algorithm`)既避免了计算二阶导数矩阵及其求逆过程，又比梯度法地收敛速度快，特别是对高维问题具有显著的优越性。

在Newton 法中的搜索方向是$- [\nabla ^2 f(x^k)]^{-1} \nabla f(x^k)$，为了不计算二阶导数极其逆阵，我们考虑构造另一矩阵用来逼近二阶导数矩阵，这类方法也称之为拟牛顿法(`Quasi-Newton Method`)。

接下来研究如何构造这样的近似矩阵，记为$\hat H^{(k)}$。要求为：每一步都能以现有的信息来确定下一个搜索方向；每做一次迭代目标函数值均有所下降；这些尽速矩阵最后应收敛于解点处的Hesse阵的逆阵。

当$f(x)$是二次函数时，其Hesse阵为常数阵A，任两点$x^k$和$x^{k+1}$处的梯度差为$\nabla f(x^{k+1}) - \nabla f(x) = A(x^{k+1} - x^k)$，即$x^{k+1} - x^k = A^{-1}[\nabla f(x^{k+1}) - \nabla f(x)]$。

对于非二次函数，依照二次函数情形，要求其Hesse阵的逆阵的第$k+1$次近似矩阵$\hat H^{(k+1)}$满足关系式：$x^{k+1} - x^k = \hat H^{(k+1)}[\nabla f(x^{k+1}) - \nabla f(x)]$。这也就是拟Newton条件。

令：

$$
\begin{cases}
\Delta G^{(k)} = \nabla f(x^{k+1}) - \nabla f(x^k) \\\\\\
\Delta x^k = x^{k+1} - x^k
\end{cases} \tag{4}
$$

则有：

$$
\Delta x^k = \hat H^{(k+1)} \Delta G{(k)} \tag{5}
$$ 

设$\hat H^{(k)}$和$\hat H^{(k+1)}$均为对称正定阵，则：

$$
\hat H^{(k+1)} = \hat H^{(k)} + \Delta \hat H^{(k)} \tag{6}
$$

其中$\Delta \hat H^{(k)}$称为第$k$次校正矩阵，由于都需要满足拟Newton条件，则有：

$$

$$
